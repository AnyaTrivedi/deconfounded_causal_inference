{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SongsRecommendationUsingDEF_3Layers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7SrMcNYlmU_",
        "outputId": "09699f28-c6d2-4ac9-e8f8-dc4c5bae8b1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=9714d22eb22353207e698c19c698f8630b31910a176b63a08fb4f3f76dbf6214\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting pyro-ppl\n",
            "  Downloading pyro_ppl-1.8.1-py3-none-any.whl (718 kB)\n",
            "\u001b[K     |████████████████████████████████| 718 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (1.21.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (3.3.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (1.11.0+cu113)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (4.64.0)\n",
            "Collecting pyro-api>=0.1.1\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.11.0->pyro-ppl) (4.2.0)\n",
            "Installing collected packages: pyro-api, pyro-ppl\n",
            "Successfully installed pyro-api-0.1.2 pyro-ppl-1.8.1\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import errno\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "!pip install wget\n",
        "import wget\n",
        "from torch.nn.functional import softplus\n",
        "\n",
        "!pip install pyro-ppl\n",
        "#!pip install pyro\n",
        "\n",
        "\n",
        "import pyro\n",
        "import pyro.optim as optim\n",
        "from pyro.contrib.easyguide import EasyGuide\n",
        "from pyro.contrib.examples.util import get_data_directory\n",
        "from pyro.distributions import Gamma, Normal, Poisson\n",
        "from pyro.infer import SVI, TraceMeanField_ELBO\n",
        "from pyro.infer.autoguide import AutoDiagonalNormal, init_to_feasible\n",
        "import pandas as pd\n",
        "torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
        "pyro.util.set_rng_seed(0)\n",
        "from scipy import sparse\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "randseed = 29266137"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper for initializing variational parameters\n",
        "def rand_tensor(shape, mean, sigma):\n",
        "    return mean * torch.ones(shape) + sigma * torch.randn(shape)\n",
        "\n",
        "class SparseGammaDEF:\n",
        "    def __init__(self,users,items):\n",
        "        # define the sizes of the layers in the deep exponential family\n",
        "        self.top_width = 10\n",
        "        self.mid_width = 4\n",
        "        self.bottom_width = 5\n",
        "\n",
        "#        self.image_size = 64 * 64\n",
        "        self.users = users\n",
        "        self.items = items\n",
        "        self.image_size = items\n",
        "\n",
        "        # define hyperparameters that control the prior\n",
        "        self.alpha_z = torch.tensor(0.1)\n",
        "        self.beta_z = torch.tensor(0.1)\n",
        "        self.alpha_w = torch.tensor(0.1)\n",
        "        self.beta_w = torch.tensor(0.3)\n",
        "\n",
        "        # define parameters used to initialize variational parameters\n",
        "        self.alpha_init = 0.5\n",
        "        self.mean_init = 0.0\n",
        "        self.sigma_init = 0.1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # define the model\n",
        "    def model(self, x):\n",
        "        x_size = x.size(0)\n",
        "\n",
        "        # sample the global weights\n",
        "        with pyro.plate(\"w_top_plate\", self.top_width * self.mid_width):\n",
        "            w_top = pyro.sample(\"w_top\", Gamma(self.alpha_w, self.beta_w))\n",
        "        with pyro.plate(\"w_mid_plate\", self.mid_width * self.bottom_width):\n",
        "            w_mid = pyro.sample(\"w_mid\", Gamma(self.alpha_w, self.beta_w))\n",
        "        with pyro.plate(\"w_bottom_plate\", self.bottom_width * self.image_size):\n",
        "            w_bottom = pyro.sample(\"w_bottom\", Gamma(self.alpha_w, self.beta_w))\n",
        "\n",
        "        # sample the local latent random variables\n",
        "        # (the plate encodes the fact that the z's for different datapoints are conditionally independent)\n",
        "        with pyro.plate(\"data\", x_size):\n",
        "            z_top = pyro.sample(\n",
        "                \"z_top\",\n",
        "                Gamma(self.alpha_z, self.beta_z).expand([self.top_width]).to_event(1),\n",
        "            )\n",
        "            # note that we need to use matmul (batch matrix multiplication) as well as appropriate reshaping\n",
        "            # to make sure our code is fully vectorized\n",
        "            w_top = (\n",
        "                w_top.reshape(self.top_width, self.mid_width)\n",
        "                if w_top.dim() == 1\n",
        "                else w_top.reshape(-1, self.top_width, self.mid_width)\n",
        "            )\n",
        "            mean_mid = torch.matmul(z_top, w_top)\n",
        "            z_mid = pyro.sample(\n",
        "                \"z_mid\", Gamma(self.alpha_z, self.beta_z / mean_mid).to_event(1)\n",
        "            )\n",
        "\n",
        "            w_mid = (\n",
        "                w_mid.reshape(self.mid_width, self.bottom_width)\n",
        "                if w_mid.dim() == 1\n",
        "                else w_mid.reshape(-1, self.mid_width, self.bottom_width)\n",
        "            )\n",
        "            mean_bottom = torch.matmul(z_mid, w_mid)\n",
        "            z_bottom = pyro.sample(\n",
        "                \"z_bottom\", Gamma(self.alpha_z, self.beta_z / mean_bottom).to_event(1)\n",
        "            )\n",
        "\n",
        "            w_bottom = (\n",
        "                w_bottom.reshape(self.bottom_width, self.image_size)\n",
        "                if w_bottom.dim() == 1\n",
        "                else w_bottom.reshape(-1, self.bottom_width, self.image_size)\n",
        "            )\n",
        "            mean_obs = torch.matmul(z_bottom, w_bottom)\n",
        "\n",
        "            # observe the data using a poisson likelihood\n",
        "            pyro.sample(\"obs\", Poisson(mean_obs).to_event(1), obs=x)\n",
        "\n",
        "    # define our custom guide a.k.a. variational distribution.\n",
        "        # (note the guide is mean field gamma)\n",
        "    def guide(self, x):\n",
        "      x_size = x.size(0)\n",
        "\n",
        "      # define a helper function to sample z's for a single layer\n",
        "      def sample_zs(name, width):\n",
        "          alpha_z_q = pyro.param(\n",
        "              \"alpha_z_q_%s\" % name,\n",
        "              lambda: rand_tensor((x_size, width), self.alpha_init, self.sigma_init),\n",
        "          )\n",
        "          mean_z_q = pyro.param(\n",
        "              \"mean_z_q_%s\" % name,\n",
        "              lambda: rand_tensor((x_size, width), self.mean_init, self.sigma_init),\n",
        "          )\n",
        "          alpha_z_q, mean_z_q = softplus(alpha_z_q), softplus(mean_z_q)\n",
        "          pyro.sample(\n",
        "              \"z_%s\" % name, Gamma(alpha_z_q, alpha_z_q / mean_z_q).to_event(1)\n",
        "          )\n",
        "\n",
        "      # define a helper function to sample w's for a single layer\n",
        "      def sample_ws(name, width):\n",
        "          alpha_w_q = pyro.param(\n",
        "              \"alpha_w_q_%s\" % name,\n",
        "              lambda: rand_tensor((width), self.alpha_init, self.sigma_init),\n",
        "          )\n",
        "          mean_w_q = pyro.param(\n",
        "              \"mean_w_q_%s\" % name,\n",
        "              lambda: rand_tensor((width), self.mean_init, self.sigma_init),\n",
        "          )\n",
        "          alpha_w_q, mean_w_q = softplus(alpha_w_q), softplus(mean_w_q)\n",
        "          pyro.sample(\"w_%s\" % name, Gamma(alpha_w_q, alpha_w_q / mean_w_q))\n",
        "\n",
        "      # sample the global weights\n",
        "      with pyro.plate(\"w_top_plate\", self.top_width * self.mid_width):\n",
        "          sample_ws(\"top\", self.top_width * self.mid_width)\n",
        "      with pyro.plate(\"w_mid_plate\", self.mid_width * self.bottom_width):\n",
        "          sample_ws(\"mid\", self.mid_width * self.bottom_width)\n",
        "      with pyro.plate(\"w_bottom_plate\", self.bottom_width * self.image_size):\n",
        "          sample_ws(\"bottom\", self.bottom_width * self.image_size)\n",
        "\n",
        "      # sample the local latent random variables\n",
        "      with pyro.plate(\"data\", x_size):\n",
        "          sample_zs(\"top\", self.top_width)\n",
        "          sample_zs(\"mid\", self.mid_width)\n",
        "          sample_zs(\"bottom\", self.bottom_width)\n",
        "\n",
        "    #def getBottomZExpectations(self):        # grab the learned variational parameters\n",
        "     # return pyro.param(\"mean_z_q_bottom\")\n",
        "\n",
        "# define a helper function to clip parameters defining the custom guide.\n",
        "# (this is to avoid regions of the gamma distributions with extremely small means)\n",
        "def clip_params():\n",
        "    for param, clip in zip((\"alpha\", \"mean\"), (-2.5, -4.5)):\n",
        "        for layer in [\"_q_top\", \"_q_mid\", \"_q_bottom\"]:\n",
        "            for wz in [\"_w\", \"_z\"]:\n",
        "                pyro.param(param + wz + layer).data.clamp_(min=clip)\n",
        "\n",
        "\n",
        "# Define a guide using the EasyGuide class.\n",
        "# Unlike the 'auto' guide, this guide supports data subsampling.\n",
        "# This is the best performing of the three guides.\n",
        "#\n",
        "# This guide is functionally similar to the auto guide, but performs\n",
        "# somewhat better. The reason seems to be some combination of: i) the better\n",
        "# numerical stability of the softplus; and ii) the custom initialization.\n",
        "# Note however that for both the easy guide and auto guide KL divergences\n",
        "# are not computed analytically in the ELBO because the ELBO thinks the\n",
        "# mean-field condition is not satisfied, which leads to higher variance gradients.\n",
        "class MyEasyGuide(EasyGuide):\n",
        "    def guide(self, x):\n",
        "        # group all the latent weights into one large latent variable\n",
        "        global_group = self.group(match=\"w_.*\")\n",
        "        global_mean = pyro.param(\n",
        "            \"w_mean\", lambda: rand_tensor(global_group.event_shape, 0.5, 0.1)\n",
        "        )\n",
        "        global_scale = softplus(\n",
        "            pyro.param(\n",
        "                \"w_scale\", lambda: rand_tensor(global_group.event_shape, 0.0, 0.1)\n",
        "            )\n",
        "        )\n",
        "        # use a mean field Normal distribution on all the ws\n",
        "        global_group.sample(\"ws\", Normal(global_mean, global_scale).to_event(1))\n",
        "\n",
        "        # group all the latent zs into one large latent variable\n",
        "        local_group = self.group(match=\"z_.*\")\n",
        "        x_shape = x.shape[:1] + local_group.event_shape\n",
        "\n",
        "        with self.plate(\"data\", x.size(0)):\n",
        "            local_mean = pyro.param(\"z_mean\", lambda: rand_tensor(x_shape, 0.5, 0.1))\n",
        "            local_scale = softplus(\n",
        "                pyro.param(\"z_scale\", lambda: rand_tensor(x_shape, 0.0, 0.1))\n",
        "            )\n",
        "            # use a mean field Normal distribution on all the zs\n",
        "            local_group.sample(\"zs\", Normal(local_mean, local_scale).to_event(1))\n"
      ],
      "metadata": {
        "id": "7JcndJVHmf7o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing the graph\n",
        "df = pd.read_csv('/content/songsDataset.csv')\n",
        "df = df.rename({\"\\'userID\\'\": \"userId\", \"\\'songID\\'\": \"movieId\", \"\\'rating\\'\": \"rating\"}, axis='columns')\n",
        "df = df.drop(labels = range(10000,2000000), axis = 0)\n",
        "songIntCode, songUniques = pd.factorize(df['movieId'], sort=True) #Reindexing songs ids\n",
        "df['movieId'] = songIntCode\n",
        "\n",
        "exposureDf = df.copy()\n",
        "exposureDf['rating'] = exposureDf['rating'].where(exposureDf['rating'] == 0, 1)\n",
        "nusers = exposureDf['userId'].nunique()\n",
        "nitems = exposureDf['movieId'].nunique()\n",
        "a_matrix = sparse.coo_matrix((exposureDf[\"rating\"],(exposureDf[\"userId\"],exposureDf[\"movieId\"])),shape=(nusers,nitems))\n",
        "a_matrix = a_matrix.todense()\n",
        "data = torch.tensor(a_matrix) #Required by our model\n",
        " \n",
        "users, items = data.shape\n",
        "print('Users Count')\n",
        "print(users)\n",
        "print('Items Count')\n",
        "print(items)\n",
        "\n",
        "print(\"data processed...\")\n",
        "\n",
        "sparse_gamma_def = SparseGammaDEF(users,items)\n",
        "\n",
        "# Due to the special logic in the custom guide (e.g. parameter clipping), the custom guide\n",
        "# seems to be more amenable to higher learning rates.\n",
        "# Nevertheless, the easy guide performs the best (presumably because of numerical instabilities\n",
        "# related to the gamma distribution in the custom guide).\n",
        "#learning_rate = 0.2 if args.guide in [\"auto\", \"easy\"] else 4.5\n",
        "learning_rate = 4.5\n",
        "#momentum = 0.05 if args.guide in [\"auto\", \"easy\"] else 0.1\n",
        "momentum = 0.1\n",
        "opt = optim.AdagradRMSProp({\"eta\": learning_rate, \"t\": momentum})\n",
        "\n",
        "# use one of our three different guide types\n",
        "# if args.guide == \"auto\":\n",
        "#     guide = AutoDiagonalNormal(sparse_gamma_def.model, init_loc_fn=init_to_feasible)\n",
        "# elif args.guide == \"easy\":\n",
        "#     guide = MyEasyGuide(sparse_gamma_def.model)\n",
        "# else:\n",
        "#     guide = sparse_gamma_def.guide\n",
        "guid_type = 'custom'\n",
        "guide = sparse_gamma_def.guide\n",
        "\n",
        "num_epochs = 30\n",
        "eval_frequency = 25\n",
        "eval_particles = 20\n",
        "\n",
        "\n",
        "# this is the svi object we use during training; we use TraceMeanField_ELBO to\n",
        "# get analytic KL divergences\n",
        "svi = SVI(sparse_gamma_def.model, guide, opt, loss=TraceMeanField_ELBO())\n",
        "\n",
        "# we use svi_eval during evaluation; since we took care to write down our model in\n",
        "# a fully vectorized way, this computation can be done efficiently with large tensor ops\n",
        "svi_eval = SVI(\n",
        "    sparse_gamma_def.model,\n",
        "    guide,\n",
        "    opt,\n",
        "    loss=TraceMeanField_ELBO(\n",
        "        num_particles=eval_particles, vectorize_particles=True\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"\\nbeginning training with %s guide...\" % guid_type)\n",
        "\n",
        "# the training loop\n",
        "for k in range(num_epochs):\n",
        "    loss = svi.step(data)\n",
        "    # for the custom guide we clip parameters after each gradient step\n",
        "    if guid_type == \"custom\":\n",
        "        clip_params()\n",
        "\n",
        "    if k % eval_frequency == 0 and k > 0 or k == num_epochs - 1:\n",
        "        loss = svi_eval.evaluate_loss(data)\n",
        "        print(\"[epoch %04d] training elbo: %.4g\" % (k, -loss))\n",
        "\n",
        "\n",
        "z_b =  pyro.param(\"mean_z_q_bottom\") #Bottom modt layer latents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY0yQnjJmvM9",
        "outputId": "61cb0fe7-0a82-4387-92bf-82708a6d5b4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Users Count\n",
            "1000\n",
            "Items Count\n",
            "7389\n",
            "data processed...\n",
            "\n",
            "beginning training with custom guide...\n",
            "[epoch 0025] training elbo: -3.577e+05\n",
            "[epoch 0029] training elbo: -1.28e+06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ratings_matrix(df, train_size=0.75):\n",
        "    user_to_row = {}\n",
        "    movie_to_column = {}\n",
        "    df_values = df.values\n",
        "    n_dims = 10\n",
        "    parameters = {}\n",
        "    \n",
        "    uniq_users = np.unique(df_values[:, 0])\n",
        "    uniq_movies = np.unique(df_values[:, 1])\n",
        "\n",
        "    for i, UserId in enumerate(uniq_users):\n",
        "        user_to_row[UserId] = i\n",
        "\n",
        "    for j, ItemId in enumerate(uniq_movies):\n",
        "        movie_to_column[ItemId] = j\n",
        "    \n",
        "    n_users = len(uniq_users)\n",
        "    n_movies = len(uniq_movies)\n",
        "    \n",
        "    R = np.zeros((n_users, n_movies))\n",
        "    \n",
        "    df_copy = df.copy()\n",
        "    train_set = df_copy.sample(frac=train_size, random_state=0)\n",
        "    test_set = df_copy.drop(train_set.index)\n",
        "    \n",
        "    for index, row in train_set.iterrows():\n",
        "        i = user_to_row[row.userId]\n",
        "        j = movie_to_column[row.movieId]\n",
        "        R[i, j] = row.rating\n",
        "\n",
        "    return R, train_set, test_set, n_dims, n_users, n_movies, user_to_row, movie_to_column\n",
        "\n",
        "R, train_set, test_set, n_dims, n_users, n_movies, user_to_row, movie_to_column = get_ratings_matrix(df, 0.8)\n",
        "\n",
        "\n",
        "\n",
        "def matrix_X(R):\n",
        "  X = []\n",
        "  for i in range(len(R)):\n",
        "    row = [1 if val == 1 else 0 for val in R[i]]\n",
        "    X.append(row)\n",
        "  return X\n",
        "\n",
        "X = matrix_X(R)\n",
        "ratings = df['rating']\n",
        "y = R\n",
        "pmfU = z_b.detach().numpy()\n",
        "\n",
        "y_scaler = preprocessing.StandardScaler().fit(y)\n",
        "y_scaled = y_scaler.fit_transform(y)\n",
        "\n",
        "X_scaler = preprocessing.StandardScaler().fit(X)\n",
        "X_scaled = X_scaler.fit_transform(X)\n",
        "\n",
        "pmfU_scaler = preprocessing.StandardScaler().fit(pmfU)\n",
        "pmfU_scaled = pmfU_scaler.fit_transform(pmfU)\n",
        "\n",
        "X_train, X_test = train_test_split(X_scaled, test_size=0.20, random_state=randseed)\n",
        "y_train, y_test = train_test_split(y_scaled, test_size=0.20, random_state=randseed)\n",
        "pmfU_train, pmfU_test = train_test_split(pmfU_scaled, test_size=0.20, random_state=randseed)\n",
        "n_users, n_items = X_train.shape"
      ],
      "metadata": {
        "id": "7t5_3QP5oNEo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "reg = linear_model.Ridge(normalize=True)\n",
        "for i in range(n_items):\n",
        "    # if i%100 == 0:\n",
        "    #   print('---- Fitting row', i, '----')\n",
        "    reg.fit(np.column_stack([X_train[:,i], pmfU_train]), y_train[:,i])\n",
        "\n",
        "\n",
        "test_items = X_test.shape[1]\n",
        "prediction = []\n",
        "\n",
        "for i in range(test_items):\n",
        "    # if i%100 == 0:\n",
        "    #   print('---- Predicting row', i, '----')\n",
        "#    print(len([X_test[:,i], pmfU_test]))\n",
        "    res = reg.predict(np.column_stack([X_test[:,i], pmfU_test]))\n",
        "    prediction.append(res)    "
      ],
      "metadata": {
        "id": "Ul3gcz63oaBc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = np.transpose(y_test)\n",
        "rmse = mean_squared_error(y_test, prediction, squared=False)\n",
        "print(rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMLdbOXpoclF",
        "outputId": "0f6c4e5d-5b3a-4bfc-8920-03885959f86a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8880783998490532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WrHHAzttolbr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}